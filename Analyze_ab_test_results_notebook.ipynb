{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "- [Additional Part - Considering timestamp](#timestamp)\n",
    "- [Bibliography](#bibliography)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.  In this test, the variant A is compared to the variant B in order to conclude which of the variants is more effective.\n",
    "\n",
    "In this project, the goal is to understand the results of an A/B test run by an e-commerce website. Specifically, the goal is to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Reading the `ab_data.csv` data and storing it in `df`.\n",
    "\n",
    "a. Reading the dataset and analyzing the top few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Finding the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 294478 rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} rows in the dataset.'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Finding the number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 290584 unique users in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique users in the dataset.'.format(df.user_id.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. Finding the proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of converted users is 0.1197.\n"
     ]
    }
   ],
   "source": [
    "prop_converted = df[df['converted']==1].shape[0]/df.shape[0]\n",
    "print('The proportion of converted users is {}.'.format(round(prop_converted,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. Finding the number of times the `new_page` and `treatment` don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of times the new_page and treatment don't match is 3893.\n"
     ]
    }
   ],
   "source": [
    "group_mismatch = df.query('landing_page==\"new_page\" and group != \"treatment\"').shape[0] + df.query('landing_page!=\"new_page\" and group == \"treatment\"').shape[0]\n",
    "print('The number of times the new_page and treatment don\\'t match is {}.'.format(group_mismatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Detect if any of the rows have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         0\n",
       "timestamp       0\n",
       "group           0\n",
       "landing_page    0\n",
       "converted       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` For the rows where **treatment** does not match with **new_page** or **control** does not match with **old_page**, we cannot be sure if this row truly received the new or old page.  \n",
    "\n",
    "a. Creating a new datafram **df2** removing rows in which treatment and control do not match with new_page and old_page respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290585, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.loc[((df['group'] == 'treatment') & (df['landing_page'] == 'new_page')) | ((df['group'] == 'control') & (df['landing_page'] == 'old_page'))].copy()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Getting some information about the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 290584 unique users in df2.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique users in df2.'.format(df.user_id.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. Is there any repeated user in **user_id**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893    773192\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_ids = df2.user_id.duplicated()\n",
    "df2[repeated_ids].user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is the row information for the repeat **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[repeated_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Removing **one** of the rows with a duplicate **user_id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290584, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop(df2[repeated_ids].index[0], inplace = True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Using **df2** to understand the data provided.\n",
    "\n",
    "a. What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of an individual converting regardless of the page they receive is 0.1196.\n"
     ]
    }
   ],
   "source": [
    "print('The probability of an individual converting regardless of the page they receive is {}.'.format(round(df2[df2['converted']==1].shape[0]/df2.shape[0],4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that an individual was in the control group, the probability they converted is 0.1204.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Given that an individual was in the control group, the probability they converted is {}.'.format(round(df2[(df2['group'] == 'control') & (df2['converted'] == 1)].shape[0]/ df2[df2['group'] == 'control'].shape[0] ,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that an individual was in the treatment group, the probability they converted is 0.1188.\n"
     ]
    }
   ],
   "source": [
    "print('Given that an individual was in the treatment group, the probability they converted is {}.'.format(round(df2[(df2['group'] == 'treatment') & (df2['converted']==1)].shape[0]/df2[df2['group'] == 'treatment'].shape[0],4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an individual received the new page is 0.5001.\n"
     ]
    }
   ],
   "source": [
    "print('The probability that an individual received the new page is {}.'.format(round(df2[df2['landing_page']=='new_page'].shape[0]/df2.shape[0],4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Considering the results from parts (a) through (d) above, a brief explanation of whether there is sufficient evidence to conclude that the new treatment page leads to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is no sufficient evidence to conclude that the new treatment page leads to more conversion.**\n",
    "\n",
    "With the previous results, it is possible to observe that around 50% of individual received the new page. Considering the probability of conversion regardless of the page received, it is obtained a value of around 11.96%, therefore, in order to have evidence that the new page lead to more conversions, we wold expect that the conversion rate for those that received the new page is higher. However, the old and new page have a conversion rate with similar numbers, with values of respectively around 12,04% and 11,88%. Those values do not give evidence that the new treatment page performs better, but that it is more likelly that the old and new page have similar conversion rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Due to the time stamp associated with each event, it is technically possible to run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "`1.` For now, a decision is made just based on all the data provided.  Assuming that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should your null and alternative hypotheses be?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case it is assumed that the old page is better unless the new page proves to be a better solution with a Type I error rate of 5%, the null and alternative hypothesis would respectively be:\n",
    "  \n",
    "$$H_0: p_{new} - p_{old} \\leq 0$$\n",
    "\n",
    "\n",
    "$$H_1: p_{new} - p_{old} > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assuming under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assuming they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "Considering a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "It is performed the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the **conversion rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversion rate for 𝑝𝑛𝑒𝑤 under the null is 0.12.\n"
     ]
    }
   ],
   "source": [
    "conversion_rate = df2[df2['converted'] == 1].shape[0]/df2.shape[0]\n",
    "print('The conversion rate for 𝑝𝑛𝑒𝑤 under the null is {}.'.format(round(conversion_rate,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the **conversion rate** for $p_{old}$ under the null? <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversion rate for 𝑝𝑜𝑙𝑑 under the null is 0.12.\n"
     ]
    }
   ],
   "source": [
    "df2[df2['converted'] == 1].shape[0]/df2.shape[0]\n",
    "print('The conversion rate for 𝑝𝑜𝑙𝑑 under the null is {}.'.format(round(conversion_rate,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is $n_{new}$, the number of individuals in the treatment group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of individuals in the treatment group is 145310.\n"
     ]
    }
   ],
   "source": [
    "n_new = df2[df2['group']=='treatment'].shape[0]\n",
    "print('The number of individuals in the treatment group is {}.'.format(n_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is $n_{old}$, the number of individuals in the control group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of individuals in the control group is 145274.\n"
     ]
    }
   ],
   "source": [
    "n_old = df2[df2['group']=='control'].shape[0]\n",
    "print('The number of individuals in the control group is {}.'.format(n_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulating $n_{new}$ transactions with a conversion rate of $p_{new}$ under the null.  Storing these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_page_converted = np.random.binomial(1, conversion_rate, n_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulating $n_{old}$ transactions with a conversion rate of $p_{old}$ under the null.  Storing these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_page_converted = np.random.binomial(1, conversion_rate, n_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find $p_{new}$ - $p_{old}$ for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between conversion rates is -0.000422.\n"
     ]
    }
   ],
   "source": [
    "conversion_new = new_page_converted.sum()/new_page_converted.shape[0]\n",
    "conversion_old = old_page_converted.sum()/old_page_converted.shape[0]\n",
    "obs_diff = conversion_new-conversion_old\n",
    "print('The difference between conversion rates is {}.'.format(round(obs_diff,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Creating 10,000 $p_{new}$ - $p_{old}$ values using the same simulation process used in parts (a) through (g) above. Storing all 10,000 values in a NumPy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diffs = []\n",
    "for _ in range(10000):\n",
    "    new_page_converted = np.random.binomial(1, conversion_rate, n_new)\n",
    "    old_page_converted = np.random.binomial(1, conversion_rate, n_old)\n",
    "    conversion_new = (new_page_converted == 1).mean()\n",
    "    conversion_old = (old_page_converted == 1).mean()\n",
    "    p_diffs.append(conversion_new-conversion_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diffs = np.array(p_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plotting a histogram of the **p_diffs**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs)\n",
    "plt.axvline(obs_diff, color = 'red')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the plot looks like a normal distribution. The mean value of the sampling distribution is zero, being slightly different from the observed difference in conversion rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of the p_diffs are greater than the actual difference observed in ab_data.csv is 0.639.\n"
     ]
    }
   ],
   "source": [
    "print('The proportion of the p_diffs are greater than the actual difference observed in ab_data.csv is {}.'.format(round((p_diffs > obs_diff).mean(),6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. Explaining what was just computed in part **j.**  What is this value called in scientific studies?  What does this value mean in terms of whether or not there is a difference between the new and old pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value that was calculated is the p-value. The p-value indicates the level of resemblance between the sample and population data, for example, a large p-value gives an indication that the sample score is similar to the population score. (A more common description of the p-value is the probability of observing the predicted result (or something more extreme in favor of the alternative hypothesis) given that the null hypothesis is true.)\n",
    "\n",
    "The p-value is often analyzed in comparison with a significance level, in which this level generally assumes a value of 0.05 (however, it depends on the type of analysis). This significance level is chosen before the beginning of the test and gives a threshold for the p-value. If the obtained p-value is smaller than the significance level, there is a significant difference between sample and population characteristics, and there is evidence to reject the null hypothesis.\n",
    "\n",
    "Considering the p-value obtained previously, we have evidence in favor of the null hypothesis and we fail to reject it. Therefore, it is provided evidence to conclude that there is no significant difference between the new and old pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We could also use a built-in to achieve similar results.  Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance.\n",
    "\n",
    "Below cells compute the number of conversions for each page, as well as the number of individuals who received each page. Let `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = df2[(df2['converted']==1) & (df2['group'] == 'control')].shape[0]\n",
    "convert_new = df2[(df2['converted']==1) & (df2['group'] == 'treatment')].shape[0]\n",
    "n_old = df2[df2['group'] == 'control'].shape[0]\n",
    "n_new = df2[df2['group'] == 'treatment'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Using `stats.proportions_ztest` to compute your test statistic and p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3109241984234394\n",
      "0.9050583127590245\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "stat, pval = proportions_ztest([convert_new,convert_old], [n_new,n_old],alternative='larger')\n",
    "print(stat)\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score indicates how many standard deviations one element is from the mean. \n",
    "\n",
    "In the above case, the z-score indicates that the conversion rate for individuals that received the new page is 1.31 standard deviations lower than those individuals that received the old page.\n",
    "\n",
    "Considering that the boundary for the rejection region with a 95% confidence interval is Z ≥ 1.96 or Z ≤ −1.96, we fail to reject the null hypothesis. Thus, we have evidence that the conversion rate is similar for both new and old pages.\n",
    "\n",
    "The p-value indicates the probability of observing the test-statistic given the null hypothesis is true. This value also leads to the same conclusion as the z-score, as we fail to reject the null hypothesis.\n",
    "\n",
    "The result from the z-test does agree with the one found in parts **j.** and **k.**. In both cases we fail to reject the null hypothesis and have evidence that the old page has a similar conversion rate as the new page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='regression'></a>\n",
    "\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` In this final part, the aim is to observe that the result achieved in the A/B test in Part II above can also be achieved by performing regression.<br><br> \n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should you be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that each row is either a conversion or no conversion, the regression to be performed in this case is the **logistic regression**, because it allows to make predictions of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use **statsmodels** to fit the regression model specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives.\n",
    "First, it is needed to create in df2 a column for the intercept, and create a dummy variable column for which page each user received.  It is included an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['intercept'] = 1\n",
    "df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Using **statsmodels** to instantiate your regression model on the two columns created in part b., then fitting the model using the two columns created in part **b.** to predict whether or not an individual converts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.Logit(df2['converted'],df2[['intercept','ab_page']])\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Providing the summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-15 09:37</td>       <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>        <td>LLR p-value:</td>      <td>0.18988</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212780.3502\n",
       "Date:               2020-04-15 09:37 BIC:              212801.5095\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           1                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290582           LLR p-value:      0.18988    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9888    0.0081  -246.6690  0.0000  -2.0046  -1.9730\n",
       "ab_page      -0.0150    0.0114    -1.3109  0.1899  -0.0374   0.0074\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value associated with **ab_page** is 0.1899, this indicates that we fail to reject the null hypothesis, which in this case is that the independent variable has no significant effect on the target variable.\n",
    "\n",
    "This value differs from the value found in in Part II because, as mentioned above, the p-value indicates if the independent variable is significant in predicting the dependent variable, whereas in Part II the p-value indicates the probability of observing the predicted result (or something more extreme in favor of the alternative hypothesis) given that the null hypothesis is true, being the null hypothesis that old and new pages result in a similar conversion rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, considering other things that might influence whether or not an individual converts.  Why it is a good idea to consider other factors to add into your regression model?  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including new factors into the regression model is a good idea because there might exist other factors that can potentially influence the dependent variable, and including them would increase predictive capability. \n",
    "\n",
    "However, this inclusion of new factors must be carefully thought, because including variables that do not contribute in obtaining better results just increase the model complexity and could also decrease model performance, decreasing accuracy and increasing runtime and memory footprint. Moreover, using multiple independent variables could raise some issues, as it can happen that:\n",
    "* a linear relationship does not exist between response and predictor\n",
    "* there are correlated errors\n",
    "* there is a non-constant variance between response and predictor\n",
    "* there is the presence of outliers\n",
    "* there is multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages, also adding an effect based on which country a user lives in. \n",
    "The data for the countries is read in the **countries.csv** dataset and merged together your datasets on the appropriate rows.\n",
    "\n",
    "Does it appear that country had an impact on conversion? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = pd.read_csv('countries.csv')\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(countries.set_index('user_id'), on = 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'CA', 'UK'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  \n",
       "0          1        0      US   0   0   1  \n",
       "1          1        0      US   0   0   1  \n",
       "2          1        1      US   0   0   1  \n",
       "3          1        1      US   0   0   1  \n",
       "4          1        0      US   0   0   1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['CA','UK','US']] = pd.get_dummies(df2.country)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212781.1253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-15 09:37</td>       <td>BIC:</td>        <td>212823.4439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>3</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290580</td>        <td>LLR p-value:</td>      <td>0.17599</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9893</td>  <td>0.0089</td>  <td>-223.7628</td> <td>0.0000</td> <td>-2.0067</td> <td>-1.9718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0149</td>  <td>0.0114</td>   <td>-1.3069</td>  <td>0.1912</td> <td>-0.0374</td> <td>0.0075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>-0.0408</td>  <td>0.0269</td>   <td>-1.5161</td>  <td>0.1295</td> <td>-0.0934</td> <td>0.0119</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>0.0099</td>   <td>0.0133</td>   <td>0.7433</td>   <td>0.4573</td> <td>-0.0162</td> <td>0.0359</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212781.1253\n",
       "Date:               2020-04-15 09:37 BIC:              212823.4439\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           3                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290580           LLR p-value:      0.17599    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9893    0.0089  -223.7628  0.0000  -2.0067  -1.9718\n",
       "ab_page      -0.0149    0.0114    -1.3069  0.1912  -0.0374   0.0075\n",
       "CA           -0.0408    0.0269    -1.5161  0.1295  -0.0934   0.0119\n",
       "UK            0.0099    0.0133     0.7433  0.4573  -0.0162   0.0359\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = sm.Logit(df2['converted'], df2[['intercept','ab_page','CA','UK']])\n",
    "result2 = model2.fit()\n",
    "result2.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the country as an independent variable in order to understand if the country has an impact on the conversion rate, it is possible to notice that **different countries do not have any impact on the conversion rate**.\n",
    "\n",
    "This can be verified by the p-value, which is higher than the previously used significance level of 0.05 and indicates that the country variable is not statistically significant in predicting if the user will convert or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. It was observed the individual factors of country and page on conversion. Now, it is regarded an interaction between page and country to see if there significant effects on conversion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>page_CA</th>\n",
       "      <th>page_UK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  page_CA  page_UK  \n",
       "0          1        0      US   0   0   1        0        0  \n",
       "1          1        0      US   0   0   1        0        0  \n",
       "2          1        1      US   0   0   1        0        0  \n",
       "3          1        1      US   0   0   1        0        0  \n",
       "4          1        0      US   0   0   1        0        0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_interaction = df2.copy()\n",
    "df2_interaction['page_CA'] = df2_interaction['ab_page']*df2_interaction['CA']\n",
    "df2_interaction['page_UK'] = df2_interaction['ab_page']*df2_interaction['UK']\n",
    "df2_interaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212782.6602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-15 09:37</td>       <td>BIC:</td>        <td>212846.1381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290578</td>        <td>LLR p-value:</td>      <td>0.19199</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9865</td>  <td>0.0096</td>  <td>-206.3440</td> <td>0.0000</td> <td>-2.0053</td> <td>-1.9676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0206</td>  <td>0.0137</td>   <td>-1.5052</td>  <td>0.1323</td> <td>-0.0473</td> <td>0.0062</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>-0.0175</td>  <td>0.0377</td>   <td>-0.4652</td>  <td>0.6418</td> <td>-0.0914</td> <td>0.0563</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>-0.0057</td>  <td>0.0188</td>   <td>-0.3057</td>  <td>0.7598</td> <td>-0.0426</td> <td>0.0311</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>page_CA</th>   <td>-0.0469</td>  <td>0.0538</td>   <td>-0.8718</td>  <td>0.3833</td> <td>-0.1523</td> <td>0.0585</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>page_UK</th>   <td>0.0314</td>   <td>0.0266</td>   <td>1.1807</td>   <td>0.2377</td> <td>-0.0207</td> <td>0.0835</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212782.6602\n",
       "Date:               2020-04-15 09:37 BIC:              212846.1381\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           5                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290578           LLR p-value:      0.19199    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9865    0.0096  -206.3440  0.0000  -2.0053  -1.9676\n",
       "ab_page      -0.0206    0.0137    -1.5052  0.1323  -0.0473   0.0062\n",
       "CA           -0.0175    0.0377    -0.4652  0.6418  -0.0914   0.0563\n",
       "UK           -0.0057    0.0188    -0.3057  0.7598  -0.0426   0.0311\n",
       "page_CA      -0.0469    0.0538    -0.8718  0.3833  -0.1523   0.0585\n",
       "page_UK       0.0314    0.0266     1.1807  0.2377  -0.0207   0.0835\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = sm.Logit(df2_interaction['converted'], df2_interaction[['intercept','ab_page','CA','UK','page_CA','page_UK']])\n",
    "result3 = model3.fit()\n",
    "result3.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering a Logistic Regression Model including the interaction between country and page variables indicated **no significant effect on the conversion rate**.\n",
    "Such conclusion can be supported by the p-values which indicate that none of the variables are statistically significant in predicting if the user will convert or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='timestamp'></a>\n",
    "\n",
    "### Addtional part - Considering timestamp\n",
    "In this additional part, it is included the timestamp in the Regression Model in order to understand if the day of the week or weekday/weekend has any influence in the conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime format\n",
    "df2['timestamp']=pd.to_datetime(df2['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get day of week\n",
    "df2['dow'] = df2['timestamp'].dt.dayofweek\n",
    "df2[['mon','tue','wed','thur','fri','sat','sun']] = pd.get_dummies(df2['dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define if it is weekday or weekend\n",
    "df2['weekday'] = df2.loc[:,'mon':'sat'].sum(axis=1)\n",
    "df2['weekend'] = df2.iloc[:,-2:-1].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212786.8299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-15 10:00</td>       <td>BIC:</td>        <td>212871.4671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>7</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290576</td>        <td>LLR p-value:</td>      <td>0.40444</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9995</td>  <td>0.0165</td>  <td>-121.2225</td> <td>0.0000</td> <td>-2.0318</td> <td>-1.9672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0149</td>  <td>0.0114</td>   <td>-1.3058</td>  <td>0.1916</td> <td>-0.0373</td> <td>0.0075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mon</th>       <td>0.0253</td>   <td>0.0211</td>   <td>1.1986</td>   <td>0.2307</td> <td>-0.0161</td> <td>0.0667</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tue</th>       <td>0.0101</td>   <td>0.0210</td>   <td>0.4821</td>   <td>0.6297</td> <td>-0.0310</td> <td>0.0513</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wed</th>       <td>0.0177</td>   <td>0.0219</td>   <td>0.8085</td>   <td>0.4188</td> <td>-0.0252</td> <td>0.0606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>thur</th>      <td>0.0142</td>   <td>0.0219</td>   <td>0.6465</td>   <td>0.5180</td> <td>-0.0288</td> <td>0.0572</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fri</th>       <td>-0.0172</td>  <td>0.0220</td>   <td>-0.7790</td>  <td>0.4360</td> <td>-0.0603</td> <td>0.0260</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sat</th>       <td>0.0223</td>   <td>0.0218</td>   <td>1.0200</td>   <td>0.3077</td> <td>-0.0205</td> <td>0.0651</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212786.8299\n",
       "Date:               2020-04-15 10:00 BIC:              212871.4671\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           7                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290576           LLR p-value:      0.40444    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9995    0.0165  -121.2225  0.0000  -2.0318  -1.9672\n",
       "ab_page      -0.0149    0.0114    -1.3058  0.1916  -0.0373   0.0075\n",
       "mon           0.0253    0.0211     1.1986  0.2307  -0.0161   0.0667\n",
       "tue           0.0101    0.0210     0.4821  0.6297  -0.0310   0.0513\n",
       "wed           0.0177    0.0219     0.8085  0.4188  -0.0252   0.0606\n",
       "thur          0.0142    0.0219     0.6465  0.5180  -0.0288   0.0572\n",
       "fri          -0.0172    0.0220    -0.7790  0.4360  -0.0603   0.0260\n",
       "sat           0.0223    0.0218     1.0200  0.3077  -0.0205   0.0651\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Logistic Regression to undersand if any specifc day of week has influence on conversion\n",
    "# the baseline day is Sunday\n",
    "model4 = sm.Logit(df2['converted'],df2[['intercept','ab_page','mon','tue','wed','thur','fri','sat']])\n",
    "result4 = model4.fit()\n",
    "result4.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the obtained p-values for each independent variable, it is possible to notice that they favor the null hypothesis and we fail to reject it, indicating that the independent variable has no significant effect on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212781.7957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-04-15 10:05</td>       <td>BIC:</td>        <td>212813.5347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>2</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290581</td>        <td>LLR p-value:</td>      <td>0.32094</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9995</td>  <td>0.0165</td>  <td>-121.2211</td> <td>0.0000</td> <td>-2.0318</td> <td>-1.9671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3115</td>  <td>0.1897</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday</th>   <td>0.0124</td>   <td>0.0167</td>   <td>0.7438</td>   <td>0.4570</td> <td>-0.0203</td> <td>0.0451</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212781.7957\n",
       "Date:               2020-04-15 10:05 BIC:              212813.5347\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           2                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290581           LLR p-value:      0.32094    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9995    0.0165  -121.2211  0.0000  -2.0318  -1.9671\n",
       "ab_page      -0.0150    0.0114    -1.3115  0.1897  -0.0374   0.0074\n",
       "weekday       0.0124    0.0167     0.7438  0.4570  -0.0203   0.0451\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Logistic Regression to undersand if weekday or weekend has influence on conversion\n",
    "# the baseline is weekend\n",
    "model5 = sm.Logit(df2['converted'],df2[['intercept','ab_page','weekday']])\n",
    "result5 = model5.fit()\n",
    "result5.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the obtained p-values for the weekday independent variable, it has no significant effect on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "The aim of the present project was to identify if individuals being presented to a certain landing page would increase the conversion rate for an e-commerce website. The main outcome of the project is to identify if a new page should be implemented, keep the old page or run the experiment for a longer period.\n",
    "\n",
    "In order to achieve such results, first, the data was organized and the DataFrame was updated. Next, some probabilities were computed in order to understand the values in the data frame and identify a possible issue for hypothesis testing. Finally, an A/B test was run in order to identify which page could lead to a higher conversion rate. This test was run considering an estimate from the null and calculating the z-score. In both cases, we fail to reject the null hypothesis and have evidence that the old page has a similar conversion rate as the new page.\n",
    "\n",
    "Also, it was performed a regression approach, in which the data was fitted to the Logistic Regression Model. First, it was considered a model with information only about the landing page (if new or old). Afterward, it was included information about the country in which the user was accessing the website and considered the interaction between country and landing page. \n",
    "Considering this approach, we failed to reject the null hypothesis in both cases (the null hypothesis was that the independent variable had no significant effect on the target variable).\n",
    "\n",
    "In addition to what was proposed in the project, the timestamp in which the user accessed the website was considered. Considering the different days of the week did not lead to any conclusion about days with higher conversion rate. Moreover, considering if weekday or weekend did not bring any further information about the user conversion.\n",
    "\n",
    "In this project, it was considered a Logistic Regression Model as there was a need for the prediction of a categorical variable. This model results in an output of a probability between 0 and 1, differently from the linear regression, in which it can output any possible value. In both models including more independent variables can lead to a better fit of the data. However, the inclusion of independent variables must be carefully thought, as it increases model complexity and can result in overfitting, which reduces the capability that the model has to generalize beyond the data for which it was fitted.\n",
    "\n",
    "As a final outcome, considering all the results obtained during this project, it is possible to conclude that **there is evidence that the old and new page leads to a similar conversion rate**. Moreover, there is no independent variable among the presented variables that are statistically significant in predicting the conversion rate if a Logistic Regression Model is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='bibliography'></a>\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "http://knowledgetack.com/python/statsmodels/proportions_ztest/\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html\n",
    "\n",
    "http://www.personal.kent.edu/~jortiz/earthstats/topic06notes.html\n",
    "\n",
    "https://medium.com/analytics-vidhya/everything-you-should-know-about-p-value-from-scratch-for-data-science-f3c0bfa3c4cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
